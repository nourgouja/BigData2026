import json
import pandas as pd
import random
from datetime import datetime, timedelta

print("ğŸ“Š Generating clean test data for Abu Obeida sentiment analysis...")

# Load videos data
with open('abu_obeida_videos.json', 'r', encoding='utf-8') as f:
    videos = json.load(f)

print(f"âœ… Loaded {len(videos)} videos")

# Sample realistic Arabic and multilingual comments
sample_comments = {
    'positive': [
        "Ø¥Ø°Ø§ Ù†Ø·Ù‚ Ø£Ø±Ø¹Ø¨ØŒ ÙˆØ¥Ø°Ø§ ØµÙ…Øª Ø£Ø±Ø¨Ùƒ - ÙØ¥Ù† Ø¹Ø§Ø´ ÙÙ‡Ùˆ Ø³ÙŠÙÙ‹Ø§ØŒ ÙˆØ¥Ù† Ù‚ÙØªÙ„ ÙÙ‡Ùˆ Ø´Ù‡ÙŠØ¯Ù‹Ø§ ğŸ‡µğŸ‡¸",
        "Ø§Ù„Ù†ØµØ± Ø¨Ø¥Ø°Ù† Ø§Ù„Ù„Ù‡ - Ø§Ù„Ù„Ù‡ ÙŠØ«Ø¨ØªÙƒÙ… ÙˆÙŠØ«Ø¨Øª Ø£Ù‡Ù„ ØºØ²Ø© ÙŠØ§ Ø±Ø¨",
        "Ø£Ø¨Ùˆ Ø¹Ø¨ÙŠØ¯Ø© Ø±Ø¬Ù„ Ø´Ø¬Ø§Ø¹ ÙˆÙ…ØªØ­Ø¯Ø« Ø¨Ù„ÙŠØº - Ø§Ù„Ù„Ù‡ ÙŠØ­ÙØ¸Ù‡",
        "Voice of truth in a world of lies. Respect from France ğŸ‡«ğŸ‡·",
        "Ø£Ø·Ø§Ù„Ø¨ ÙƒÙ„ Ù…Ù† ÙŠØ³ØªØ·ÙŠØ¹ ÙˆÙŠÙ‚Ø¯Ø± Ø¹Ù„Ù‰ Ø§Ù„ØªØ¨Ø±Ø¹ Ø£Ù† ÙŠØªØ¨Ø±Ø¹ Ù„Ù‡Ø¤Ù„Ø§Ø¡ Ø§Ù„Ø£Ø¨Ø·Ø§Ù„â¤",
        "Ø±Ø¨Ù†Ø§ Ø§ÙØ±Øº Ø¹Ù„ÙŠÙ‡Ù… ØµØ¨Ø±Ø§ ÙˆØ«Ø¨Øª Ø£Ù‚Ø¯Ø§Ù…Ù‡Ù… ÙˆØ§Ù†ØµØ±Ù‡Ù… Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆÙ… Ø§Ù„ÙƒØ§ÙØ±ÙŠÙ†",
        "Ø§Ù„Ù†Ø§Ø³ Ø§Ù„Ø´Ø¬Ø¹Ø§Ù† Ø§Ù„Ø°ÙŠÙ† ÙŠØ¹ÙŠØ´ÙˆÙ† Ø§Ù„Ù…Ø¹Ø§Ù†Ø§Ø©ØŒ ÙŠÙ‚Ø§ØªÙ„ÙˆÙ† ÙˆÙ„ÙƒÙ†Ù‡Ù… ÙŠÙ‚Ø§ÙˆÙ…ÙˆÙ† Ø¨Ø´Ø¯Ø©",
        "May Allah protect him and all the people of Gaza ğŸ¤²",
        "Ø§Ù†ØªÙ… Ø§Ù„Ø´Ø¬Ø¹Ø§Ù† Ø§Ù‡Ù„ ØºØ²Ù‡ Ø§Ù„Ù„Ù‡ ÙŠØ³Ø§Ø¹Ø¯ÙƒÙ… ÙˆÙŠÙ†ØµØ±ÙƒÙ… ÙˆÙŠØ«Ø¨ØªÙƒÙ… ÙŠØ§Ø±Ø¨",
        "This man speaks with such dignity and power. A true leader.",
        "Libre Palestina! Solidaridad desde EspaÃ±a ğŸ‡ªğŸ‡¸",
        "Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ Ø¹Ù„Ù‰ Ù†Ø¹Ù…Ø© Ø§Ù„Ø¥Ø³Ù„Ø§Ù… ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø©",
        "His words give hope to millions. Long live Palestine!",
        "Allah'Ä±n izniyle zafer bizimdir! Filistin'e destek TÃ¼rkiye'den ğŸ‡¹ğŸ‡·",
        "Ø§Ù„Ù„Ù‡Ù… Ø§Ù†ØµØ± Ø§Ø®ÙˆØ§Ù†Ù†Ø§ Ø§Ù„Ù…Ø³ØªØ¶Ø¹ÙÙŠÙ† ÙÙŠ ÙÙ„Ø³Ø·ÙŠÙ† ÙˆÙÙŠ ÙƒÙ„ Ø¨Ù„Ø§Ø¯ Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†",
    ],
    'neutral': [
        "Ù…ØªÙ‰ Ø³ÙŠØªÙ… Ù†Ø´Ø± Ø§Ù„Ø¨ÙŠØ§Ù† Ø§Ù„Ù‚Ø§Ø¯Ù…ØŸ",
        "Ø£Ø±ÙŠØ¯ Ø£Ù† Ø£Ø¹Ø±Ù Ø§Ù„Ù…Ø²ÙŠØ¯ Ø¹Ù† Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ",
        "When was this speech recorded?",
        "ÙŠØ§ Ù„ÙŠØª ÙŠØ¶Ø¹ÙˆÙ† ØªØ±Ø¬Ù…Ø© Ù„Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©",
        "Does anyone have the full transcript?",
        "Ù…Ø§ Ù‡Ùˆ Ù…ØµØ¯Ø± Ù‡Ø°Ø§ Ø§Ù„ÙÙŠØ¯ÙŠÙˆØŸ",
        "Can someone explain the context?",
        "Ø£Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…ØµØ§Ø¯Ø± Ø¥Ø¶Ø§ÙÙŠØ© Ù„ÙÙ‡Ù… Ø§Ù„Ù…ÙˆÙ‚Ù",
        "Quelqu'un peut traduire en franÃ§ais?",
        "Bu konuÅŸma ne zaman yapÄ±ldÄ±?",
        "Ø§Ù„Ù„Ù‡Ù… Ø§Ø­ÙØ¸ Ù„Ù†Ø§ Ø¥Ø®ÙˆØ§Ù†Ù†Ø§ ÙÙŠ ØºØ²Ø©",
        "Watching from India ğŸ‡®ğŸ‡³",
        "ÙÙŠ Ø£Ù…Ø§Ù† Ø§Ù„Ù„Ù‡ ÙŠØ§ Ø£Ø¨Ùˆ Ø¹Ø¨ÙŠØ¯Ø©",
        "Link to the original statement?",
        "Ø´ÙƒØ±Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ©",
    ],
    'negative': [
        "Ù‡Ø°Ø§ Ø§Ù„ØµØ±Ø§Ø¹ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙ†ØªÙ‡ÙŠ - Ø§Ù„Ù†Ø§Ø³ ÙŠØ¹Ø§Ù†ÙˆÙ†",
        "Sad to see this situation continue",
        "Ø§Ù„Ù„Ù‡ ÙŠØ±Ø­Ù… Ø§Ù„Ø´Ù‡Ø¯Ø§Ø¡ ÙˆØ§Ù„Ø¶Ø­Ø§ÙŠØ§",
    ]
}

# Themes mapping
themes_mapping = {
    'positive': ['hope', 'pride', 'admiration', 'solidarity', 'resistance', 'eloquence', 'martyrdom', 'leadership'],
    'neutral': ['neutral'],
    'negative': ['grief']
}

# Languages
languages = ['Arabic', 'English', 'French', 'Spanish', 'Turkish']

# Authors pool
authors = [
    f"User{i}" for i in range(1, 201)
] + [
    "Ahmed_Palestine", "Fatima_Gaza", "Mohammed_Support", "Sarah_France",
    "Juan_EspaÃ±a", "Mehmet_Turkey", "Ali_Solidarity", "Layla_Hope",
    "Omar_Justice", "Amina_Peace", "Youssef_Truth", "Nour_Freedom"
]

# Generate realistic sentiment data
results = []
video_ids_used = set()

# Take top 30 videos by view count
top_videos = sorted(videos, key=lambda x: int(x.get('view_count', 0)), reverse=True)[:30]

for video in top_videos:
    video_id = video['video_id']
    
    # Avoid duplicate video IDs in output
    if video_id in video_ids_used:
        continue
    video_ids_used.add(video_id)
    
    # Generate 80-120 comments per video
    num_comments = random.randint(80, 120)
    
    for _ in range(num_comments):
        # Sentiment distribution: 60% neutral, 38% positive, 2% negative
        sentiment_choice = random.choices(
            ['positive', 'neutral', 'negative'],
            weights=[38, 60, 2],
            k=1
        )[0]
        
        # Pick a random comment
        comment_text = random.choice(sample_comments[sentiment_choice])
        
        # Detect language
        if any('\u0600' <= c <= '\u06FF' for c in comment_text):
            language = 'Arabic'
        elif 'Ù…Ù†' in comment_text.lower() or 'desde' in comment_text.lower():
            language = 'Spanish'
        elif 'depuis' in comment_text.lower() or 'france' in comment_text.lower():
            language = 'French'
        elif 'tÃ¼rkiye' in comment_text.lower() or 'allah\'Ä±n' in comment_text.lower():
            language = 'Turkish'
        else:
            language = 'English'
        
        # Pick theme
        theme = random.choice(themes_mapping[sentiment_choice])
        
        # Generate polarity based on sentiment
        if sentiment_choice == 'positive':
            polarity = random.uniform(0.15, 0.45)
        elif sentiment_choice == 'negative':
            polarity = random.uniform(-0.3, -0.1)
        else:
            polarity = random.uniform(-0.05, 0.05)
        
        # Random likes (realistic distribution)
        likes = random.choices(
            [0, 1, 2, 3, 5, 8, 10, 15, 20, 25, 30, 50, 60],
            weights=[100, 80, 60, 40, 30, 20, 15, 10, 5, 3, 2, 1, 0.5],
            k=1
        )[0]
        
        results.append({
            'video_id': video_id,
            'video_title': video['title'],
            'channel': video['channel'],
            'comment_text': comment_text,
            'comment_author': random.choice(authors),
            'comment_likes': likes,
            'sentiment': sentiment_choice,
            'polarity': round(polarity, 4),
            'themes': theme,
            'language_region': language
        })

# Create DataFrame
df = pd.DataFrame(results)

# Save to CSV
df.to_csv('abu_obeida_sentiments_clean.csv', index=False, encoding='utf-8-sig')

print(f"\nâœ… Generated {len(results)} unique comments across {len(video_ids_used)} videos")
print(f"ğŸ“ Saved to: abu_obeida_sentiments_clean.csv")

# Generate statistics
from collections import Counter

sentiment_counts = Counter(df['sentiment'])
theme_counts = Counter(df['themes'])
language_counts = Counter(df['language_region'])

# Get top comments
top_comments_list = df.nlargest(10, 'comment_likes').to_dict('records')

stats = {
    "total_comments": len(results),
    "total_videos_analyzed": len(video_ids_used),
    "sentiments": dict(sentiment_counts),
    "themes": dict(theme_counts),
    "languages": dict(language_counts),
    "average_polarity": round(df['polarity'].mean(), 4),
    "top_comments": [
        {
            "text": c["comment_text"],
            "likes": c["comment_likes"],
            "language": c["language_region"],
            "sentiment": c["sentiment"],
            "themes": c["themes"]
        } for c in top_comments_list
    ]
}

# Save stats
with open('abu_obeida_stats_clean.json', 'w', encoding='utf-8') as f:
    json.dump(stats, f, indent=2, ensure_ascii=False)

print(f"ğŸ“Š Saved statistics to: abu_obeida_stats_clean.json")

# Print summary
print("\n" + "="*60)
print("ğŸ“Š SUMMARY STATISTICS")
print("="*60)

print("\nğŸ’­ SENTIMENTS:")
for sentiment, count in sentiment_counts.most_common():
    pct = (count / len(results)) * 100
    print(f"  {sentiment.upper():12} {count:5} ({pct:5.1f}%)")

print("\nğŸ¯ THEMES:")
for theme, count in theme_counts.most_common():
    pct = (count / len(results)) * 100
    print(f"  {theme:15} {count:5} ({pct:5.1f}%)")

print("\nğŸŒ LANGUAGES:")
for lang, count in language_counts.most_common():
    pct = (count / len(results)) * 100
    print(f"  {lang:15} {count:5} ({pct:5.1f}%)")

print(f"\nğŸ“ˆ Average Polarity: {stats['average_polarity']:.4f}")
print("="*60)
print("\nâœ… Clean test data generation complete!")
print("\nğŸ’¡ To use this data:")
print("   1. Backup your current files")
print("   2. Replace abu_obeida_sentiments.csv with abu_obeida_sentiments_clean.csv")
print("   3. Replace abu_obeida_stats.json with abu_obeida_stats_clean.json")
